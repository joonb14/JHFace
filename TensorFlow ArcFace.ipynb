{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow ArcFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ref https://github.com/peteryuX/arcface-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from models import ArcFaceModel\n",
    "# from losses import SoftmaxLoss\n",
    "from losses import softmax_loss\n",
    "import dataset\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import logging\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"s\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\r\n"
     ]
    }
   ],
   "source": [
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4658122\n",
      "36391\n",
      "1164531\n",
      "9097\n"
     ]
    }
   ],
   "source": [
    "### IJB-C Dataset\n",
    "\n",
    "# batch_size = 128\n",
    "# input_size = 112\n",
    "# embd_shape = 512\n",
    "# head_type = 'ArcHead'\n",
    "# backbone_type = 'MobileNetV2'\n",
    "# w_decay=5e-4\n",
    "# num_classes = 3584 \n",
    "# base_lr = 0.01\n",
    "# dataset_len = 13033 \n",
    "# epochs = 100\n",
    "# steps_per_epoch = dataset_len // batch_size\n",
    "\n",
    "### MS1M dataset\n",
    "\n",
    "batch_size = 128 # Initially 128\n",
    "input_size = 112\n",
    "embd_shape = 512\n",
    "head_type = 'ArcHead' # ''ArcHead', CosHead', 'SphereHead'\n",
    "# Backbones w/ pretrained weights:\n",
    "#     MobileNet, MobileNetV2, InceptionResNetV2, InceptionV3, ResNet50, ResNet50V2, ResNet101V2, NASNetLarge, NASNetMobile, Xception\n",
    "#     But if you are trying to use NasNet, please check this issue first: https://github.com/keras-team/keras-applications/issues/78\n",
    "#         We manually download the weight file and explicitly load it in models.py file\n",
    "# Backbones w/o pretrained weights:\n",
    "#     MobileNetV3Large, MobileNetV3Small, EfficientNetLite0~6, EfficientNetB0~7\n",
    "backbone_type = 'EfficientNetLite0' \n",
    "w_decay=5e-4\n",
    "num_classes = 85742 \n",
    "dataset_len = 5822653\n",
    "if head_type == 'SphereHead':\n",
    "    base_lr = 0.01 \n",
    "#     margin = 1.35\n",
    "#     logist_scale = 30.0 \n",
    "    margin = 4\n",
    "    logist_scale = 1\n",
    "elif head_type == 'CosHead':\n",
    "    base_lr = 0.01 \n",
    "    margin=0.35\n",
    "    logist_scale=64\n",
    "elif head_type == 'ArcHead':\n",
    "    base_lr = 0.01 \n",
    "    margin=0.5\n",
    "    logist_scale=64\n",
    "else:\n",
    "    base_lr = 0.01 # initially 0.01\n",
    "epochs = 20\n",
    "save_steps = 1000\n",
    "train_size = int(0.8 * dataset_len)\n",
    "print(train_size)\n",
    "steps_per_epoch = train_size // batch_size\n",
    "print(steps_per_epoch)\n",
    "val_size = dataset_len - train_size\n",
    "print(val_size)\n",
    "validation_steps = val_size // batch_size\n",
    "print(validation_steps)\n",
    "steps = 1\n",
    "is_ccrop=False\n",
    "binary_img=True\n",
    "is_Adam = False   # True\n",
    "\n",
    "dgx = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"arcface_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, 112, 112, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "efficientnet-lite0 (Functional) (None, 4, 4, 1280)   3413024     input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "OutputLayer (Functional)        (None, 512)          10493440    efficientnet-lite0[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "label (InputLayer)              [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ArcHead (Functional)            (None, 85742)        43899904    OutputLayer[0][0]                \n",
      "                                                                 label[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 57,806,368\n",
      "Trainable params: 57,760,768\n",
      "Non-trainable params: 45,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# train_data_dir = \"/raid/workspace/jbpark/IJB-C_Asian/\"\n",
    "# tfrecord_name = train_data_dir+'ijbc_bin.tfrecord'\n",
    "if dgx:\n",
    "    train_data_dir = \"/raid/workspace/jbpark/ms1m/\"\n",
    "    tfrecord_name = train_data_dir+'ms1m_bin.tfrecord'\n",
    "else:\n",
    "    train_data_dir = \"/hd/jbpark/dataset/ms1m/\"\n",
    "    tfrecord_name = train_data_dir+'ms1m_bin.tfrecord'\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = dataset.load_tfrecord_dataset(\n",
    "    tfrecord_name, batch_size, train_size=train_size, binary_img=binary_img, \n",
    "    is_ccrop=is_ccrop)\n",
    "\n",
    "# print(\"data: \", train_dataset)\n",
    "\n",
    "if dgx:\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:2\"])\n",
    "else:\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
    "#     strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "\n",
    "with strategy.scope():\n",
    "    model = ArcFaceModel(size=input_size,\n",
    "                             backbone_type=backbone_type,\n",
    "                             num_classes=num_classes,\n",
    "                             margin=margin, \n",
    "                             logist_scale=logist_scale,\n",
    "                             head_type=head_type,\n",
    "                             embd_shape=embd_shape,\n",
    "                             w_decay=w_decay,\n",
    "                             training=True)\n",
    "    model.summary()\n",
    "\n",
    "    learning_rate = tf.constant(base_lr)\n",
    "    if is_Adam:\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate)\n",
    "\n",
    "    loss_fn = softmax_loss\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 1000/36391 [..............................] - ETA: 1:37:38 - loss: 46.4101\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_46.313289642333984.ckpt\n",
      " 2000/36391 [>.............................] - ETA: 1:31:08 - loss: 46.2927\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_46.02696990966797.ckpt\n",
      " 3000/36391 [=>............................] - ETA: 1:27:03 - loss: 46.1537\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_45.72218704223633.ckpt\n",
      " 4000/36391 [==>...........................] - ETA: 1:26:00 - loss: 46.0067\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_45.41082763671875.ckpt\n",
      " 5000/36391 [===>..........................] - ETA: 1:23:12 - loss: 45.8560\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_45.096778869628906.ckpt\n",
      " 6000/36391 [===>..........................] - ETA: 1:20:03 - loss: 45.7037\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_44.789031982421875.ckpt\n",
      " 7000/36391 [====>.........................] - ETA: 1:17:08 - loss: 45.5515\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_44.48915481567383.ckpt\n",
      " 8000/36391 [=====>........................] - ETA: 1:15:14 - loss: 45.4005\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_44.19991683959961.ckpt\n",
      " 9000/36391 [======>.......................] - ETA: 1:12:19 - loss: 45.2516\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_43.92295837402344.ckpt\n",
      "10000/36391 [=======>......................] - ETA: 1:09:35 - loss: 45.1051\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_43.65174865722656.ckpt\n",
      "11000/36391 [========>.....................] - ETA: 1:06:38 - loss: 44.9611\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_43.39353561401367.ckpt\n",
      "12000/36391 [========>.....................] - ETA: 1:03:45 - loss: 44.8200\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_43.141334533691406.ckpt\n",
      "13000/36391 [=========>....................] - ETA: 1:00:57 - loss: 44.6813\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_42.893760681152344.ckpt\n",
      "14000/36391 [==========>...................] - ETA: 58:12 - loss: 44.5449\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_42.65297317504883.ckpt\n",
      "15000/36391 [===========>..................] - ETA: 55:42 - loss: 44.4108\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_42.41533279418945.ckpt\n",
      "16000/36391 [============>.................] - ETA: 53:20 - loss: 44.2788\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_42.18235397338867.ckpt\n",
      "17000/36391 [=============>................] - ETA: 50:35 - loss: 44.1487\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_41.95217514038086.ckpt\n",
      "18000/36391 [=============>................] - ETA: 47:55 - loss: 44.0204\n",
      "Epoch 00001: saving model to /raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/ms1m_EfficientNetLite0_ArcHead_check/SGD/e_1_l_41.726585388183594.ckpt\n",
      "18831/36391 [==============>...............] - ETA: 45:39 - loss: 43.9150"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if dgx:\n",
    "    base_dir = \"/raid/workspace/honghee/FaceRecognition/\"\n",
    "    # save_name = \"ms1m_mobilenet_check/\"\n",
    "    save_name = \"ms1m_\"+backbone_type+\"_\"+head_type+\"_check/\"\n",
    "else:\n",
    "    base_dir = \"/hd/honghee/models/\"\n",
    "    save_name = \"ms1m_\"+backbone_type+\"_\"+head_type+\"_check/\"\n",
    "\n",
    "if is_Adam:\n",
    "    version = \"Adam\"\n",
    "else:\n",
    "    version = \"SGD\"\n",
    "\n",
    "Path(base_dir + 'checkpoints/w_tfidentity/' + save_name + version).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "### MS1M dataset\n",
    "tb_callback = TensorBoard(log_dir='logs/arcface/',\n",
    "                                  update_freq = batch_size * 5,\n",
    "                                  profile_batch=0)\n",
    "tb_callback._total_batches_seen = steps\n",
    "tb_callback._samples_seen = steps * batch_size\n",
    "mc_callback = ModelCheckpoint(\n",
    "            base_dir + 'checkpoints/w_tfidentity/' + save_name + version +'/e_{epoch}_l_{loss}.ckpt',\n",
    "            save_freq = save_steps, verbose=1,\n",
    "            save_weights_only=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "#                               patience=0.1, min_lr=0.001)\n",
    "# callbacks = [mc_callback, tb_callback,reduce_lr]\n",
    "callbacks = [mc_callback, tb_callback]\n",
    "\n",
    "### IJB-C Dataset\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint(\n",
    "#         base_dir+\"checkpoints/\"+save_name+\".ckpt\", \n",
    "# #         monitor='val_accuracy', \n",
    "#         monitor='loss', \n",
    "#         verbose=1, \n",
    "#         save_best_only=True, \n",
    "#         save_weights_only = True,\n",
    "#         mode='min'\n",
    "#     ),\n",
    "#     EarlyStopping(\n",
    "# #         monitor='val_accuracy', \n",
    "#         monitor='loss', \n",
    "#         patience=15, \n",
    "#         min_delta=0.001, \n",
    "#         mode='min'\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs, \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume training with latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "if dgx:\n",
    "    base_dir = \"/raid/workspace/honghee/FaceRecognition/checkpoints/w_tfidentity/\"\n",
    "    # save_name = \"ms1m_mobilenet_check/SGD/*\"\n",
    "    save_name = \"ms1m_\"+backbone_type+\"_\"+head_type+\"_check/\"+version+\"/*\"\n",
    "else:\n",
    "    base_dir = \"/hd/honghee/models/checkpoints/w_tfidentity/\"\n",
    "    save_name = \"ms1m_\"+backbone_type+\"_\"+head_type+\"_check/\"+version+\"/*\"\n",
    "file_list = []\n",
    "for files in glob(base_dir+save_name):\n",
    "    file_list.append(files.split('/')[-1].split('l_')[-1])\n",
    "file_list.sort()\n",
    "\n",
    "load_file_name = []\n",
    "for files in glob(base_dir+save_name):\n",
    "    if file_list[0] == files.split('/')[-1].split('l_')[-1]:\n",
    "        load_file_name = files\n",
    "best_checkpoint = load_file_name.split('.data')[0]\n",
    "initial_epoch = int(load_file_name.split('e_')[-1].split('_')[0])-1\n",
    "print(initial_epoch)\n",
    "print(best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if dgx:\n",
    "    base_dir = \"/raid/workspace/honghee/FaceRecognition/\"\n",
    "    # save_name = \"ms1m_mobilenet_check/\"\n",
    "    save_name = \"ms1m_\"+backbone_type+\"_\"+head_type+\"_check/\"\n",
    "else:\n",
    "    base_dir = \"/hd/honghee/models/\"\n",
    "    save_name = \"ms1m_\"+backbone_type+\"_\"+head_type+\"_check/\"\n",
    "\n",
    "if is_Adam:\n",
    "    version = \"Adam\"\n",
    "else:\n",
    "    version = \"SGD\"\n",
    "\n",
    "Path(base_dir + 'checkpoints/w_tfidentity/' + save_name + version).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# train_data_dir = \"/raid/workspace/jbpark/IJB-C_Asian/\"\n",
    "# tfrecord_name = train_data_dir+'ijbc_bin.tfrecord'\n",
    "\n",
    "if dgx:\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:2\"])\n",
    "else:\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n",
    "#     strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
    "\n",
    "if dgx:\n",
    "    train_data_dir = \"/raid/workspace/jbpark/ms1m/\"\n",
    "    tfrecord_name = train_data_dir+'ms1m_bin.tfrecord'\n",
    "else:\n",
    "    train_data_dir = \"/hd/jbpark/dataset/ms1m/\"\n",
    "    tfrecord_name = train_data_dir+'ms1m_bin.tfrecord'\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = dataset.load_tfrecord_dataset(\n",
    "    tfrecord_name, batch_size, train_size=train_size, binary_img=binary_img,\n",
    "    is_ccrop=is_ccrop)\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    model = ArcFaceModel(size=input_size,\n",
    "                             backbone_type=backbone_type,\n",
    "                             num_classes=num_classes,\n",
    "                             head_type=head_type,\n",
    "                             embd_shape=embd_shape,\n",
    "                             w_decay=w_decay,\n",
    "                             training=True)\n",
    "    model.load_weights(best_checkpoint)\n",
    "\n",
    "    learning_rate = tf.constant(base_lr)\n",
    "    if is_Adam:\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate)\n",
    "\n",
    "    loss_fn = softmax_loss\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    model.summary()\n",
    "    \n",
    "tb_callback = TensorBoard(log_dir='logs/arcface/',\n",
    "                                  update_freq = batch_size * 5,\n",
    "                                  profile_batch=0)\n",
    "tb_callback._total_batches_seen = steps\n",
    "tb_callback._samples_seen = steps * batch_size\n",
    "mc_callback = ModelCheckpoint(\n",
    "            base_dir + 'checkpoints/w_tfidentity/' + save_name + version +'/e_{epoch}_l_{loss}.ckpt',\n",
    "            save_freq = save_steps, verbose=1,\n",
    "            save_weights_only=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "#                               patience=1, min_lr=0.001)\n",
    "# callbacks = [mc_callback, tb_callback,reduce_lr]\n",
    "callbacks = [mc_callback, tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs, \n",
    "    initial_epoch=initial_epoch,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
